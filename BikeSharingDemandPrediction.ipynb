{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fadakenitin/Regression_BikeSharingDemandPrediction/blob/main/BikeSharingDemandPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Bike Sharing Demand Prediction\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name**             - Nitin Fadake"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seoul Bike Sharing Demand Prediction\n",
        "\n",
        "The Seoul Bike Sharing Demand Prediction project aimed to develop machine learning models to predict the demand for bike rentals in Seoul. The project utilized the Seoul Bike Sharing dataset, which contained information about various factors such as weather conditions, date, time, and the number of bikes rented.\n",
        "\n",
        "The project followed a systematic approach, starting with data preprocessing and exploratory data analysis (EDA). During EDA, important insights were gathered about the dataset, including the distribution of variables, correlations between features, and trends in bike rental patterns.\n",
        "\n",
        "Feature engineering played a crucial role in creating meaningful predictors for the models. New features were derived from existing ones, such as extracting the day of the week, hour of the day, and season from the date and time variables. Additionally, categorical variables were encoded using one-hot encoding to make them suitable for model training.\n",
        "\n",
        "The dataset was then split into training and testing sets. Various machine learning models were implemented, including Linear Regression, Decision Tree Regression, Random Forest Regression, and XGBoost Regression. The models were trained on the training set and evaluated using evaluation metrics such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R-squared (R2) score.\n",
        "\n",
        "To improve the models' performance, hyperparameter tuning was performed using techniques such as GridSearchCV and RandomizedSearchCV. These techniques explored different combinations of hyperparameters to find the optimal configuration that yielded the best results.\n",
        "\n",
        "The evaluation of the models revealed that the XGBoost Regression model achieved the highest accuracy, with an R2 score of 0.933. This indicated that the model could explain 93.3% of the variance in the bike rental demand. The tuned Decision Tree Regression and Random Forest Regression models also showed improvements in their R2 scores compared to their initial implementations.\n",
        "\n",
        "The project concluded that the developed machine learning models could effectively predict bike rental demand in Seoul. These models can assist bike-sharing companies in optimizing their operations, allocating resources efficiently, and enhancing the overall user experience. By accurately estimating demand, companies can manage inventory, plan maintenance schedules, and make data-driven decisions to meet customer needs.\n",
        "\n",
        "However, it is important to note that the success of the models relies on the availability of high-quality data and the periodic retraining and updating of the models as new data becomes available. Regular evaluation and monitoring are crucial to ensure the continued effectiveness of the models.\n",
        "\n",
        "Overall, the Seoul Bike Sharing Demand Prediction project demonstrated the value of machine learning in the bike-sharing industry. The developed models provide actionable insights that can drive business decisions and improve operational efficiency, ultimately contributing to the success of bike-sharing companies and enhancing the biking experience for users in Seoul."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/fadakenitin/Regression_BikeSharingDemandPrediction\n"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To develop a machine learning model that can forecast the number of bikes that will be rented at a given time, enabling bike sharing service providers to optimize their operations and meet customer demand efficiently."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import io"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "scores_files=files.upload()"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv(io.BytesIO(scores_files['SeoulBikeData (1).csv']),encoding='ISO-8859-1')"
      ],
      "metadata": {
        "id": "ebcsEromehGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "UXNKsmsvg8Mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "data.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "data.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "data.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import missingno as mnso\n",
        "mnso.matrix(data)"
      ],
      "metadata": {
        "id": "nJuPq8CwirPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1.The Seoul Bike Sharing Demand dataset contains bike rental data in Seoul, South Korea.\n",
        "The dataset is complete, with no missing values, ensuring reliable analysis.\n",
        "\n",
        "2.Important columns in the dataset include date, time, temperature, humidity, wind speed, visibility, and the number of rented bikes.\n",
        "\n",
        "3.The target variable is the \"Rented Bike Count,\" representing the number of bike rentals for a given time period.\n",
        "\n",
        "4.The dataset enables analysis of bike rental patterns and their correlation with weather conditions.\n",
        "\n",
        "5.Machine learning models can be built using this dataset to predict bike demand and optimize bike sharing systems.\n",
        "\n",
        "6.Insights from the dataset can help improve resource allocation and operational efficiency in bike sharing services.\n",
        "\n",
        "7.The dataset contributes to sustainable transportation solutions by enabling data-driven decision-making.\n",
        "\n",
        "8.Researchers, data scientists, and stakeholders can leverage the dataset to study bike sharing dynamics and make informed improvements.\n",
        "\n",
        "9.The dataset provides valuable information for enhancing the bike sharing experience for users and operators.\n",
        "\n",
        "10.It serves as a valuable resource for understanding and improving bike sharing services in urban environments."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "data.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "data.describe(include='all').T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Date: The date when bike rentals were recorded.\n",
        "\n",
        "2.Rented Bike Count: The number of bikes rented, which is the target variable to predict.\n",
        "\n",
        "3.Hour: The hour of the day when bike rentals were recorded.\n",
        "\n",
        "4.Temperature(°C): The temperature in Celsius at the time of recording.\n",
        "\n",
        "5.Humidity(%): The relative humidity at the time of recording.\n",
        "\n",
        "6.Wind speed (m/s): The wind speed in meters per second at the time of recording.\n",
        "\n",
        "7.Visibility (10m): The visibility in meters at the time of recording.\n",
        "\n",
        "8.Dew point temperature(°C): The dew point temperature in Celsius at the time  \n",
        "of recording.\n",
        "\n",
        "9.Solar Radiation (MJ/m2): The amount of solar radiation in megajoules per square meter at the time of recording.\n",
        "\n",
        "10.Rainfall(mm): The amount of rainfall in millimeters at the time of recording.\n",
        "\n",
        "11.Snowfall (cm): The amount of snowfall in centimeters at the time of recording.\n",
        "\n",
        "12.Seasons: The season when bike rentals were recorded (e.g., spring, summer, autumn, winter).\n",
        "\n",
        "13.Holiday: Indicates whether the recorded date is a holiday or not.\n",
        "\n",
        "14.Functioning Day: Indicates whether the recorded day is a functioning day or not."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "data.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready."
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data['Date']=pd.to_datetime(data['Date'])"
      ],
      "metadata": {
        "id": "13KMJwnvnQxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I9CJyKXb2jXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()\n",
        "## Check either converted into datetime format or not ##"
      ],
      "metadata": {
        "id": "JuwsUw-PzaV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract Day, Month and Year from date column\n",
        "data['Weekday']=data['Date'].dt.day_name()\n",
        "data['Day']=data['Date'].dt.day\n",
        "data['Month']=data['Date'].dt.month\n",
        "data['Year']=data['Date'].dt.year"
      ],
      "metadata": {
        "id": "kqmxzm9Vo6RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "KCzQKC872tm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "H0dwYYXfpfTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(['Date'],inplace=True,axis=1)"
      ],
      "metadata": {
        "id": "_BdTmQzYpl5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code provided shows several data manipulations performed on the dataset. Here's a breakdown of each step:\n",
        "\n",
        "1. Converting the 'Date' column to datetime format:\n",
        "   - The 'Date' column in both the 'df' and 'original_data' DataFrames is converted to datetime format using the `pd.to_datetime()` function. This allows for easier manipulation and analysis of date-related information.\n",
        "\n",
        "2. Creating new columns:\n",
        "   - 'Weekday': A new column named 'Weekday' is created in the 'df' DataFrame using the 'Date' column. It extracts the day name (e.g., Monday, Tuesday) from the date using the `dt.day_name()` function.\n",
        "   - 'Day': Another new column named 'Day' is created in the 'df' DataFrame using the 'Date' column. It extracts the day value (e.g., 1, 2, 3) from the date using the `dt.day` function.\n",
        "   - 'Month': A new column named 'Month' is created in the 'df' DataFrame using the 'Date' column. It extracts the month value (e.g., 1, 2, 3) from the date using the `dt.month` function.\n",
        "   - 'Year': A new column named 'Year' is created in the 'df' DataFrame using the 'Date' column. It extracts the year value (e.g., 2019, 2020) from the date using the `dt.year` function.\n",
        "\n",
        "3. Dropping the 'Date' column:\n",
        "   - The 'Date' column is dropped from the 'df' DataFrame using the `drop()` function with the 'axis' parameter set to 1 (indicating column-wise operation) and the 'inplace' parameter set to True (indicating the modification is made directly to the DataFrame).\n",
        "\n",
        "Insights and analysis:\n",
        "The manipulations performed on the dataset provide additional features related to the date, such as the weekday, day, month, and year. These new columns allow for a more detailed analysis of the bike rental patterns based on different time dimensions.\n",
        "\n",
        "With the added columns, you can now explore insights such as:\n",
        "- The distribution of bike rentals across weekdays (e.g., higher rentals on weekdays compared to weekends).\n",
        "- Monthly trends in bike rentals (e.g., higher rentals during summer months).\n",
        "- Yearly variations in bike rentals (e.g., increasing or decreasing demand over the years).\n",
        "\n",
        "These insights can help identify patterns and seasonality in the bike rental data, enabling better decision-making for resource allocation, marketing strategies, and operational planning in the bike sharing system."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "a=data.groupby(['Month'])['Rented Bike Count'].sum()\n",
        "plt.plot(a)\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Total Rented Bike Count\")\n",
        "plt.title(\"Rented Bike Count Per Month\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have selected this chart to find rented count for each month. Also this chart allowing you to compare the counts across different months."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this chart highest demands in the month of June and lowest demand in the month of the February.\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Insights gained from this chart help to build the positive business impact by taking some measures to increase the demands in of-peak demand months for example giving some offers."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2\n",
        "\n"
      ],
      "metadata": {
        "id": "CyU7CBTbUEjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(10,6))\n",
        "Day=data.groupby(['Day']).sum().reset_index()\n",
        "sns.barplot(x='Day',y=\"Rented Bike Count\",data=Day)"
      ],
      "metadata": {
        "id": "g99DbZk-URp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?\n",
        "\n"
      ],
      "metadata": {
        "id": "cKcaQNyKVUAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar plot is an appropriate choice for visualizing this type of data because it allows for easy comparison of values between different categories (in this case, different days). The height of each bar represents the magnitude of the total rented bike count for that specific day, making it straightforward to observe any differences or patterns. A bar plot is an appropriate choice for visualizing this type of data because it allows for easy comparison of values between different categories (in this case, different days). The height of each bar represents the magnitude of the total rented bike count for that specific day, making it straightforward to observe any differences or patterns."
      ],
      "metadata": {
        "id": "jZj48QCaWjs2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?\n",
        "\n"
      ],
      "metadata": {
        "id": "eq921-daVV0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Peak and Off-Peak Days: By analyzing the heights of the bars, we can identify which days have higher demand for rented bikes (peak days) and which days have lower demand (off-peak days)."
      ],
      "metadata": {
        "id": "8kVF2XgwWqr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "c8ZE0_NSWbuc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifying High-Demand Days: If the insights reveal certain days with consistently high rented bike counts, the business can allocate more resources (bikes, staff, marketing) to meet the increased demand on those days. This proactive approach can lead to improved customer satisfaction, increased revenue, and positive business growth.\n",
        "\n",
        "Seasonal Planning: If the insights show clear seasonal variations in the rented bike counts, the business can plan and adjust operations accordingly. For example, during peak seasons, they can offer special promotions, expand the bike fleet, or adjust pricing strategies to maximize revenue during high-demand periods."
      ],
      "metadata": {
        "id": "ero1I-XRWf_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "e=data.groupby(['Seasons'])['Rented Bike Count'].sum()\n",
        "plt.pie(e,labels=e.index,autopct='%1.1f%%')\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.groupby(['Seasons'])['Rented Bike Count'].sum()"
      ],
      "metadata": {
        "id": "fDO0E7V4VdQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get comparison of the rented bike count distribution among seasons"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Highest Season: The largest slice in the pie chart corresponds to the \"Summer\" season, indicating that it has the highest proportion of the rented bike count among all seasons.\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights regarding the distribution of rented bike count among seasons can assist in resource allocation. The business can allocate resources such as bikes, staff, and marketing efforts more efficiently during peak seasons (e.g., Summer) to meet the higher demand. This can result in improved customer satisfaction and increased revenue.\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=data, x=data['Month'], y='Rented Bike Count')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Rented Bike Count')\n",
        "plt.title('Rented Bike Count Distribution by Month')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To identify any variations, outliers, or seasonality"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This box plot showcases the distribution of rented bike counts for each month, helping identify any variations, outliers, or seasonality."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These plots allow you to analyze and understand the rented bike count data from different perspectives, helping you identify patterns, trends, and variations in demand. These insights can be useful for decision-making, resource allocation, and developing strategies to optimize business operations, marketing, and customer satisfaction."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "x=data.groupby(['Hour'])['Rented Bike Count'].sum()\n",
        "plt.bar(x.index,x.values)\n",
        "plt.title('Distribution of the demands as per hours')"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find time of the day having maximum demands."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Minimum demad hours : 4-5 AM\n",
        "\n",
        "Maximum demand hours : 5-7 PM"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights will help create a positive business impact. They can be used to maximize revenue by increasing the rental price during peak hours, where there is high demand for bike rentals. Additionally, the rental price can be decreased during non-peak hours to attract more customers and ensure optimal utilization of the bike-sharing service."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "sns.countplot(data=data, x=\"Holiday\")\n",
        "plt.xlabel(\"Holiday\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Distribution of Holiday\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar chart helps us to easily identify the number of Holidays and No Holiday present in the dataset"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset contains more No Holidays than the Holidays."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The countplot you provided analyzes the distribution of holidays in the dataset. While this plot alone may not directly provide insights on bike rental demand or growth, it can be a useful visualization for understanding the occurrence and distribution of holidays in the dataset."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "\n",
        "# Extract the day of the week from the date\n",
        "\n",
        "\n",
        "# Group the dataset by day of the week and calculate the sum of \"Rented Bike Count\" for each day\n",
        "grouped_data = data.groupby(\"Weekday\")[\"Rented Bike Count\"].sum()\n",
        "\n",
        "# Create a bar plot of rented bike count by day of the week\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(grouped_data.index, grouped_data.values)\n",
        "plt.xlabel(\"Day of the Week\")\n",
        "plt.ylabel(\"Rented Bike Count\")\n",
        "plt.title(\"Rented Bike Count by Day of the Week\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar chart make it easy to analyze the variation in demand on different days of the week"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sunday has lowest demands as compared to other days."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights regarding the rented bike count by day of the week can assist in resource allocation. By understanding the patterns of higher demand on weekdays and lower demand on weekends, the business can allocate resources such as bikes, staff, and operational support more efficiently. This can result in improved customer satisfaction, optimized resource utilization, and increased revenue."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "average_demand_by_season = data.groupby('Seasons')['Rented Bike Count'].mean()\n",
        "plt.figure(figsize=(8, 6))\n",
        "average_demand_by_season.plot(kind='bar')\n",
        "plt.xlabel('Season')\n",
        "plt.ylabel('Average Demand')\n",
        "plt.title('Average Bike Sharing Demand by Season')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I picked the bar chart showing the average bike sharing demand by season because it allows us to compare the average demand across different seasons, providing a clear visual representation of the differences in demand patterns.\n",
        "\n"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals the varying levels of bike sharing demand across different seasons.\n",
        "It helps identify which seasons experience higher or lower demand for bike sharing."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes by Understanding the seasonal variations in demand and plan their operations accordingly, such as adjusting fleet sizes, staffing, and marketing efforts.\n",
        "Optimize resource allocation and ensure availability of bikes and infrastructure during peak demand seasons.\n"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(data['Rented Bike Count'])\n",
        "plt.xlabel('Rented Bike Count')\n",
        "plt.title('Boxplot of Bike Sharing Demand')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the boxplot to analyze the distribution of bike sharing demand. The boxplot displays statistical information such as the median, quartiles, and potential outliers. It provides insights into the central tendency, spread, and skewness of the demand data."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights gained from the boxplot:\n",
        "\n",
        "Central tendency: The median line in the boxplot represents the median demand, indicating the typical demand level.\n",
        "\n",
        "Spread: The box's height indicates the interquartile range (IQR), showing the range within which most of the data falls. The whiskers extend to the minimum and maximum values within a certain range.\n",
        "\n",
        "Outliers: Any data points outside the whiskers are considered outliers and may be worth investigating for potential anomalies."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifying outliers can help uncover unusual demand patterns or errors in data recording.\n",
        "Understanding the distribution and spread of demand can assist in capacity planning, resource allocation, and inventory management.\n",
        "Comparing boxplots across different periods or segments (e.g., seasons) can reveal variations in demand patterns and guide decision-making."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "plt.figure(figsize=(20, 6))\n",
        "sns.violinplot(data=data, x='Weekday', y='Rented Bike Count')\n",
        "plt.xlabel('Week')\n",
        "plt.ylabel('Rented Bike Count')\n",
        "plt.title('Violin Plot of Bike Sharing Demand by Week')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I chose the violin plot to analyze the bike sharing demand by week because it provides a combination of a boxplot and a kernel density plot, offering a more comprehensive understanding of the distribution of demand across different weeks."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights gained from this violin chart:\n",
        "\n",
        "Distribution of demand: The width of the violins represents the density of demand values. Wider areas indicate higher density and frequent demand levels, while narrower areas indicate lower density and less frequent demand levels.\n",
        "\n",
        "Central tendency: The white dot inside each violin represents the median demand for that week.\n",
        "\n",
        "Shape variations: The shape of the violins reveals the skewness of the demand distribution. A symmetric shape suggests a balanced distribution, while asymmetry indicates potential skewness.\n",
        "\n",
        "Outliers: Any points outside the violins represent potential outliers in demand for a specific week."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifying demand distribution patterns for each week can assist in resource allocation, staffing, and inventory management on a weekly basis.\n",
        "Understanding the shape of the violins can indicate demand patterns, such as a skewed distribution during certain weeks, which may require targeted interventions or adjustments to operations."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "corr_matrix = data.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the correlation heatmap to examine the relationships between different variables in the dataset. The heatmap allows for a visual representation of the correlation coefficients, providing insights into the strength and direction of the relationships."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation strength: The color intensity in the heatmap represents the strength of the correlation between pairs of variables. Darker shades indicate a stronger correlation, while lighter shades suggest a weaker or no correlation.\n",
        "\n",
        "Positive and negative correlations: The color spectrum, ranging from cool to warm colors, indicates the direction of the correlation. Cool colors (e.g., blue) represent negative correlations, while warm colors (e.g., red) represent positive correlations."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(data)\n",
        "plt.title('Pair Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the pair plot to visualize the pairwise relationships between multiple variables in the dataset. The pair plot provides a matrix of scatterplots for each pair of variables, allowing for a comprehensive analysis of their relationships."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights gained from the pair plot:\n",
        "\n",
        "Scatterplots: Each scatterplot in the pair plot shows the relationship between two variables. It helps visualize patterns, trends, and potential correlations between variables.\n",
        "\n",
        "Diagonal plots: The diagonal of the pair plot represents the distribution of each variable. It provides insights into the individual variable's distribution and can reveal any potential outliers or skewed distributions."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): There is no significant correlation between temperature and the number of rented bikes.\n",
        "\n",
        "Alternative Hypothesis (H1): There is a significant correlation between temperature and the number of rented bikes."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats as stats\n",
        "\n",
        "temperature = data['Temperature(°C)']\n",
        "rented_bike_count = data['Rented Bike Count']\n",
        "\n",
        "corr, p_value = stats.pearsonr(temperature, rented_bike_count)\n",
        "\n",
        "alpha = 0.05\n",
        "\n",
        "if p_value < alpha:\n",
        "    conclusion = \"Reject the null hypothesis. There is a significant correlation between temperature and the number of rented bikes.\"\n",
        "else:\n",
        "    conclusion = \"Fail to reject the null hypothesis. There is no significant correlation between temperature and the number of rented bikes.\"\n",
        "\n",
        "print(conclusion)"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearson Correlation Test"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the Pearson correlation test for Hypothesis 1 because it is commonly used to measure the strength and direction of the linear relationship between two continuous variables. In this case, the hypothesis was related to the correlation between temperature (a continuous variable) and the number of rented bikes (also a continuous variable).\n"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): The average number of rented bikes is the same during different seasons.\n",
        "\n",
        "Alternative Hypothesis (HA): The average number of rented bikes differs between different seasons."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "seasons = [\"Spring\", \"Summer\", \"Fall\", \"Winter\"]\n",
        "season_data = [data[data[\"Seasons\"] == season][\"Rented Bike Count\"] for season in seasons]\n",
        "\n",
        "f_stat, p_value = f_oneway(*season_data)\n",
        "\n",
        "# Print the F-statistic and p-value\n",
        "print(\"F-statistic:\", f_stat)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Determine the conclusion based on the p-value\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANOVA test"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ANOVA test was chosen for this hypothesis because it is suitable for comparing the means of multiple groups (in this case, the different seasons). ANOVA allows us to determine if there is a significant difference in the means of the groups and identify which groups, if any, have significantly different means. Since we are comparing the average number of rented bikes across different seasons."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): The average number of rented bikes is the same during weekdays and weekends.\n",
        "\n",
        "Alternative Hypothesis (HA): The average number of rented bikes differs between weekdays and weekends."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Separate the data into weekdays and weekends\n",
        "weekday_data = data[data[\"Functioning Day\"] == \"Yes\"]\n",
        "weekend_data = data[data[\"Functioning Day\"] == \"No\"]\n",
        "\n",
        "# Perform independent two-sample t-test\n",
        "t_stat, p_value = ttest_ind(weekday_data[\"Rented Bike Count\"], weekend_data[\"Rented Bike Count\"], equal_var=False)\n",
        "\n",
        "# Print the t-statistic and p-value\n",
        "print(\"T-statistic:\", t_stat)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Determine the conclusion based on the p-value\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent two-sample-t-test."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I chose the independent two-sample t-test for Hypothesis 3 because it is commonly used to compare the means of two independent groups. In this case, the hypothesis involved comparing the bike sharing demand between weekdays and weekends, which are two distinct and independent groups."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "data.isna().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " There are no any missing value in any of the column , therefore i haven't use any missing value imputational technique."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "data.skew().sort_values(ascending=True)"
      ],
      "metadata": {
        "id": "HYUhCVzo-3vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "symmetric_feature=[]\n",
        "non_symmetric_feature=[]\n",
        "for i in data.describe().columns:\n",
        "  if abs(data[i].mean()-data[i].median())<0.2:\n",
        "    symmetric_feature.append(i)\n",
        "  else:\n",
        "    non_symmetric_feature.append(i)\n",
        "\n",
        "# Getting Symmetric Distributed Features\n",
        "print(\"Symmetric Distributed Features : -\",symmetric_feature)\n",
        "\n",
        "# Getting Skew Symmetric Distributed Features\n",
        "print(\"Skew Symmetric Distributed Features : -\",non_symmetric_feature)\n",
        "\n",
        "# Removing Customer Service Calls column from the list as it's an important factor\n",
        "# which can't be treated as outliers here will is already leading to higher churn as we have seen furing analysis.\n",
        "non_symmetric_feature.pop()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Symmetric features defining upper and lower boundry\n",
        "def outlier_treatment(data,feature):\n",
        "  upper_boundary= data[feature].mean()+3*data[feature].std()\n",
        "  lower_boundary= data[feature].mean()-3*data[feature].std()\n",
        "  return upper_boundary,lower_boundary"
      ],
      "metadata": {
        "id": "sWYrYVEqxNCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restricting the data to lower and upper boundry\n",
        "for feature in symmetric_feature:\n",
        "  data.loc[data[feature]<= outlier_treatment(data=data,feature=feature)[1], feature]=outlier_treatment(data=data,feature=feature)[1]\n",
        "  data.loc[data[feature]>= outlier_treatment(data=data,feature=feature)[0], feature]=outlier_treatment(data=data,feature=feature)[0]"
      ],
      "metadata": {
        "id": "3mjaSBfBdWVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def outlier_treatment_skew(df,feature):\n",
        "  IQR= data[feature].quantile(0.75)- data[feature].quantile(0.25)\n",
        "  lower_bridge =data[feature].quantile(0.25)-3*IQR\n",
        "  upper_bridge =data[feature].quantile(0.25)+3*IQR\n",
        "  return upper_bridge,lower_bridge"
      ],
      "metadata": {
        "id": "IkjXmFdmYO9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restricting the data to lower and upper boundry\n",
        "for feature in non_symmetric_feature:\n",
        "  data.loc[data[feature]<= outlier_treatment_skew(df=data,feature=feature)[1], feature]=outlier_treatment_skew(df=data,feature=feature)[1]\n",
        "  data.loc[data[feature]>= outlier_treatment_skew(df=data,feature=feature)[0], feature]=outlier_treatment_skew(df=data,feature=feature)[0]"
      ],
      "metadata": {
        "id": "yHvuABfqd_Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After Outlier Treatment showing the dataset distribution using strip plot\n",
        "# Visualising  code for the numerical columns\n",
        "for col in data.describe().columns:\n",
        "  fig=plt.figure(figsize=(9,6))\n",
        "  sns.stripplot(data[col])"
      ],
      "metadata": {
        "id": "zrVCi1Yad_BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I separated the skew-symmetric and symmetric features and defined the upper and lower boundaries as follows. Since it is a classification problem, I restricted both boundaries and pulled down the higher values to the upper limit. For example, if a person is churning with a value of 150 for any column, but the IQR and upper boundary range is 140, it is quite possible that they will also churn for a value of 140. Therefore, I restricted the value to 140, which is the upper boundary.\n",
        "\n",
        "Another approach is to drop extreme values, but I didn't do that due to the small number of data points available.\n",
        "\n",
        "In a Gaussian distribution, which is a symmetric curve, outliers may be present. In such cases, we can set the boundaries by considering the standard deviation.\n",
        "\n",
        "The box plot is a useful graphical display for describing the behavior of data in the middle as well as at the ends of the distributions. It uses the median and the lower and upper quartiles (defined as the 25th and 75th percentiles). The interquartile range (IQR), which is the difference between Q3 and Q1, is used to construct the box plot. To identify extreme values in the tails of the distribution, the following quantities (called fences) are used:\n",
        "\n",
        "- Lower inner fence: Q1 - 1.5 * IQR\n",
        "- Upper inner fence: Q3 + 1.5 * IQR\n",
        "- Lower outer fence: Q1 - 3 * IQR\n",
        "- Upper outer fence: Q3 + 3 * IQR\n"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "ATc8Y3L5o72_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# Specify the categorical fetures to be encoded\n",
        "categorical_features = ['Seasons', 'Holiday', 'Functioning Day','Weekday']"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Holiday'].value_counts()"
      ],
      "metadata": {
        "id": "b43Q39NqEVLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Holiday']=data['Holiday'].map({'No Holiday':0,'Holiday':1})"
      ],
      "metadata": {
        "id": "3iSqXfT9q_WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Functioning Day'].value_counts()"
      ],
      "metadata": {
        "id": "oGkJIqV5rPGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Functioning Day']=data['Functioning Day'].map({'Yes':1,'No':0})"
      ],
      "metadata": {
        "id": "KqkP85PGrYck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Seasons'].value_counts()"
      ],
      "metadata": {
        "id": "4I5pRvMxrudo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Weekday'].value_counts()"
      ],
      "metadata": {
        "id": "EFBucoaHr3ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_seasons=pd.get_dummies(data['Seasons'],drop_first=True)\n",
        "data_weekday=pd.get_dummies(data['Weekday'],drop_first=True)"
      ],
      "metadata": {
        "id": "pSRWEz6-r9jZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "rdXsAutqs-lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.concat([data,data_seasons,data_weekday],axis=1)"
      ],
      "metadata": {
        "id": "DXw8i82ytaqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "K6ZPzlpttt1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# weekday and seasons should be removed from the data because we etract different features from these columns\n",
        "data.drop(['Seasons','Weekday'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "DNyQnxV7uGzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "Vhrx7gypuq1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "JsqyV6nbu__O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-hot encoding creates binary columns for each unique category in the original variable, where a value of 1 represents the presence of that category, and 0 represents the absence. This technique is suitable when there is no inherent order or relationship between the categories.\n",
        "\n",
        "The reason for using one-hot encoding is that it allows machine learning algorithms to interpret and utilize categorical data effectively. By converting categorical variables into numerical features, we enable algorithms to perform mathematical operations on the data and capture any patterns or relationships that may exist."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)\n",
        "\n",
        "Based on the provided dataset, it appears that the dataset contains a mix of numerical and categorical variables. The categorical variables have already been encoded using one-hot encoding, as indicated by the additional columns with binary values.\n",
        "\n",
        "If you are planning to use this dataset for machine learning tasks, the current preprocessing steps seem sufficient. The numerical variables are already in a suitable format for modeling, and the categorical variables have been encoded to numerical representations.\n",
        "\n",
        "There is no need of data preprocessing."
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "def get_vif(data):\n",
        "  vif =pd.DataFrame()\n",
        "  vif['variables']=data.columns\n",
        "  vif['VIF']=[variance_inflation_factor(data.values,i) for i in range(data.shape[1])]\n",
        "  return vif\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "not_for_vif = ['Day','Month','Year','Rented Bike Count']\n",
        "\n",
        "get_vif(data[[i for i in data.describe().columns if i not in not_for_vif]])\n"
      ],
      "metadata": {
        "id": "-jvSEYYsh7q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(['Dew point temperature(°C)'],inplace=True , axis =1)"
      ],
      "metadata": {
        "id": "If3pWNLWmV96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  The get_vif() function calculates the VIF for each feature in the dataset.\n",
        "VIF measures the correlation between a predictor variable and the other predictor variables in a linear regression model. It helps identify multicollinearity, which occurs when predictor variables are highly correlated with each other.\n",
        "The VIF value quantifies the extent of multicollinearity. Higher VIF values indicate stronger multicollinearity, suggesting that the variable may not provide unique or independent information to the model.\n",
        "\n",
        "- Implementation of VIF:\n",
        "\n",
        "  The get_vif() function creates an empty DataFrame called 'vif' to store the results.\n",
        "It iterates through each column in the 'data' DataFrame and calculates the VIF using the variance_inflation_factor() function from the 'statsmodels.stats.outliers_influence' module.\n",
        "The VIF value is then appended to the 'vif' DataFrame along with the corresponding variable name.\n",
        "The function returns the 'vif' DataFrame.\n",
        "\n",
        "- Excluding certain variables:\n",
        "\n",
        "  The 'not_for_vif' list contains the variables ('Day', 'Month', 'Year','Rented Bike Count') that should be excluded from the VIF calculation.\n",
        "The get_vif() function is called on a subset of the 'df' DataFrame, excluding the variables mentioned in 'not_for_vif'.\n",
        "\n",
        "- Dropping a variable:\n",
        "\n",
        "  The line of code 'df.drop(['Dew point temperature(°C)'], axis=1, inplace=True)' drops the 'Dew point temperature(°C)' column from the 'df' DataFrame.\n",
        "  This suggests that the 'Dew point temperature(°C)' variable might have been identified as having high multicollinearity with other variables based on its VIF value.\n",
        "  The feature selection method used in this code is VIF, which helps identify and eliminate variables with high multicollinearity. By removing variables that are highly correlated with each other, we can improve the model's performance and interpretability."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Date: The date column is often important in time-series analysis as it allows for the exploration of temporal patterns and seasonality in bike rentals. It can help identify trends, weekly patterns, and seasonal variations in bike demand.\n",
        "\n",
        "Weather-related columns: Variables such as temperature, humidity, wind speed, and visibility can have a significant impact on bike demand. These weather factors can influence people's willingness to ride bikes, affecting their comfort and safety. Analyzing their relationship with bike rentals can provide insights into weather-dependent demand patterns.\n",
        "\n",
        "Weekday: The weekday column, derived from the date, can be important as it captures the day of the week. Different days of the week may exhibit distinct patterns in bike rental demand. For example, weekdays may have higher demand during commuting hours, while weekends or specific weekdays (such as Fridays) may have higher recreational demand.\n",
        "\n",
        "Holiday or special event indicators: If available in the dataset, columns indicating holidays or special events can be crucial in understanding bike demand. Public holidays, festivals, or local events can significantly influence bike rental patterns due to changes in commuting habits, leisure activities, or increased tourism.\n",
        "\n",
        "Rented Bike Count (target variable): The target variable itself is important as it represents the number of rented bikes. Analyzing the relationship between the target variable and other features can help identify their impact on bike demand and build predictive models."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "x=data.drop(['Rented Bike Count'],axis=1)\n",
        "y=data['Rented Bike Count']"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import package\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=20)"
      ],
      "metadata": {
        "id": "Fmk81NM7y5Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape of the training and testing dataset\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "aKhfga1z0aFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data splitting ratio used is test_size=0.2, which means that 20% of the data will be used for testing and 80% will be used for training.\n",
        "\n",
        "This is a commonly used splitting ratio in machine learning tasks.\n",
        "The reason for choosing a 80:20 ratio is to strike a balance between having enough data for training the model and having enough data for evaluating its performance. The larger the training set, the more data the model has to learn from and potentially generalize well. However, it's also important to have a sufficient amount of data for testing to assess the model's performance on unseen examples."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "#import package\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc=StandardScaler()\n",
        "sc.fit(x_train)\n",
        "x_train=sc.transform(x_train)\n",
        "x_test=sc.transform(x_test)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc.scale_"
      ],
      "metadata": {
        "id": "q9Bd6vyE57gQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc.mean_"
      ],
      "metadata": {
        "id": "hCwMTfHU57HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale your data and why?\n"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The StandardScaler method from the sklearn.preprocessing module is used for scaling the data. The StandardScaler method standardizes the features by subtracting the mean and scaling to unit variance.\n",
        "\n",
        "The choice of using StandardScaler for scaling the data is a common practice in machine learning. It is particularly useful when the features in the dataset have different scales and variances. By applying standardization, the features are transformed to have a mean of 0 and a standard deviation of 1, which brings all the features to a similar scale. This helps in preventing features with larger scales from dominating the learning process and ensures that all features contribute equally.\n",
        "\n",
        "\n",
        "\n",
        "Using StandardScaler for scaling the data helps in improving the performance of certain machine learning algorithms that are sensitive to the scale of the features, such as linear models, support vector machines (SVMs), and neural networks."
      ],
      "metadata": {
        "id": "-ztfEIeNFZIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model 1- Linear Regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()\n",
        "# Fit the Algorithm\n",
        "lr.fit(x_train, y_train)\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=lr.predict(x_test)"
      ],
      "metadata": {
        "id": "9IowtXwPPIDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "en265NAsPX8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score\n",
        "mse = mean_squared_error(y_test,y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test,y_pred)\n",
        "r2 = r2_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "CaObl5SFgOws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mse)\n",
        "print(rmse)\n",
        "print(mae)\n",
        "print(r2)"
      ],
      "metadata": {
        "id": "XT77SVYpgerU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metrics(y_true,y_pred,model_name):\n",
        "  mse = mean_squared_error(y_test,y_pred)\n",
        "  rmse = np.sqrt(mse)\n",
        "  mae = mean_absolute_error(y_test,y_pred)\n",
        "  r2 = r2_score(y_test,y_pred)\n",
        "\n",
        "  print(f'{model_name}:[mse:{round(mse,3)},rmse:{round(rmse,3)},mae:{round(mae,3)},r2:{round(r2,3)}]')\n"
      ],
      "metadata": {
        "id": "q8qWDQqyggs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_metrics(y_test,y_pred,'LinearRegression')"
      ],
      "metadata": {
        "id": "aKNNKPPLgjpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Visualizing evaluation Metric Score chart\n",
        "plt.scatter(y_test,y_pred)\n",
        "plt.xlabel('Ground Truth')\n",
        "plt.ylabel('Prediction')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Fit the Algorithm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the hyperparameters and their possible values\n",
        "parameters = {'fit_intercept': [True, False],\n",
        "              'positive': [True, False]}\n",
        "\n",
        "# Create an instance of the Linear Regression model\n",
        "lr = LinearRegression()\n",
        "\n",
        "# Create the Grid Search object\n",
        "grid_search = GridSearchCV(lr, parameters, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Fit the Grid Search object to the training data\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Use the best model for prediction\n",
        "best_lr = grid_search.best_estimator_\n",
        "y_pred = best_lr.predict(x_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"Root Mean Squared Error:\", rmse)\n",
        "print(\"Mean Absolute Error:\", mae)\n",
        "print(\"R2 Score:\",r2)\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV technique\n",
        "\n",
        "I chose GridSearchCV because it is a simple yet effective method for hyperparameter tuning. It performs an exhaustive search over all possible combinations of hyperparameters, making it more likely to find the optimal set of hyperparameters. Additionally, it uses cross-validation to evaluate the model's performance, which helps in reducing the risk of overfitting.\n"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no improvement in the accuracy of the model."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2  Decision Tree"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "# Fit the Algorithm\n",
        "dtr = DecisionTreeRegressor().fit(x_train,y_train)\n",
        "y_pred_dtr = dtr.predict(x_test)\n",
        "\n",
        "# Predict on the model\n",
        "get_metrics(y_test,y_pred_dtr,'DecisionTreeRegressor')"
      ],
      "metadata": {
        "id": "Y-onR8sRVlzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.scatter(y_test,y_pred_dtr)\n",
        "plt.xlabel('Ground Truth')\n",
        "plt.ylabel('Prediction')\n",
        "plt.title('Decision Tree')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Fit the Algorithm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'max_depth': [None, 5, 10, 15],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "}\n",
        "\n",
        "# Create the Decision Tree Regressor model\n",
        "dtr = DecisionTreeRegressor()\n",
        "\n",
        "# Create the Grid Search object\n",
        "grid_search = GridSearchCV(estimator=dtr, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the Grid Search object to the training data\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and best estimator\n",
        "best_params = grid_search.best_params_\n",
        "best_estimator = grid_search.best_estimator_\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(best_params)\n",
        "\n",
        "# Use the best estimator for prediction\n",
        "y_pred_dtr_tuned = best_estimator.predict(x_test)\n",
        "\n",
        "# Calculate evaluation metrics with tuned model\n",
        "get_metrics(y_test, y_pred_dtr_tuned, \"DecisionTree (Tuned)\")\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose Grid Search because it is a simple yet effective method for hyperparameter tuning. It systematically explores all possible combinations of hyperparameters in the specified grid and evaluates each combination using cross-validation. This allows us to find the best hyperparameters that maximize the model's performance on the given evaluation metric."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, hyperparameter tuning using Grid Search has led to improvements in the model's performance. Here is the updated evaluation metric score chart:\n",
        "\n",
        "Decision Tree (Before Hyperparameter Tuning):\n",
        "\n",
        "MSE: 80025.019\n",
        "\n",
        "RMSE: 282.887\n",
        "\n",
        "MAE: 160.056\n",
        "\n",
        "R2: 0.803\n",
        "\n",
        "Decision Tree (After Hyperparameter Tuning):\n",
        "\n",
        "MSE: 63172.418\n",
        "\n",
        "RMSE: 251.341\n",
        "\n",
        "MAE: 151.031\n",
        "\n",
        "R2: 0.844\n",
        "\n",
        "After hyperparameter tuning, the Decision Tree model shows improvements in all evaluation metrics. The MSE, RMSE, and MAE have decreased, indicating better prediction accuracy, while the R2 score has increased, indicating a better fit to the data."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Mean Squared Error (MSE):\n",
        "- MSE measures the average squared difference between the predicted and actual values.\n",
        "- Lower MSE indicates that the model's predictions are closer to the actual values on average.\n",
        "- In a business context, a lower MSE suggests that the model's predictions have smaller errors, leading to more accurate forecasts or estimates.\n",
        "- The business impact is improved accuracy in decision-making, such as better resource allocation, inventory management, or demand forecasting.\n",
        "\n",
        "2. Root Mean Squared Error (RMSE):\n",
        "- RMSE is the square root of MSE and represents the average magnitude of the prediction errors.\n",
        "- Similar to MSE, a lower RMSE indicates better prediction accuracy and smaller errors.\n",
        "- In a business context, a lower RMSE implies that the model's predictions have smaller deviations from the actual values.\n",
        "- The business impact is enhanced reliability in forecasting, reducing potential errors in planning, budgeting, or risk assessment.\n",
        "\n",
        "3. Mean Absolute Error (MAE):\n",
        "- MAE measures the average absolute difference between the predicted and actual values.\n",
        "- Lower MAE indicates that, on average, the model's predictions are closer to the actual values.\n",
        "- In a business context, a lower MAE suggests that the model's predictions have smaller absolute deviations from the actual values.\n",
        "- The business impact is improved precision in estimating outcomes or targets, leading to more accurate decision-making and resource allocation.\n",
        "\n",
        "4. R-squared (R2) Score:\n",
        "- R2 score represents the proportion of the variance in the dependent variable (target) that is predictable from the independent variables (features) in the model.\n",
        "- Higher R2 score indicates that a larger portion of the variance in the target variable is explained by the model's features.\n",
        "- In a business context, a higher R2 score implies that the model captures a larger portion of the underlying patterns and relationships in the data.\n",
        "- The business impact is increased confidence in the model's predictive power and its ability to explain the target variable, leading to more reliable decision-making and insights."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 Random Forest"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# Fit the Algorithm\n",
        "rfr=RandomForestRegressor().fit(x_train,y_train)\n",
        "y_pred_rfr=rfr.predict(x_test)\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_metrics(y_test,y_pred_rfr,'RandomForestRegressor')"
      ],
      "metadata": {
        "id": "K5hf2kajijHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.scatter(y_test,y_pred_rfr)\n",
        "plt.xlabel('Ground Truth')\n",
        "plt.ylabel('Prediction')\n",
        "plt.title('Random Forest Regressor')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Fit the Algorithm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
        "    'max_depth': [None, 5, 10],  # Maximum depth of the trees\n",
        "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
        "    'max_features': ['auto', 'sqrt']  # Number of features to consider when looking for the best split\n",
        "}\n",
        "\n",
        "# Create the Random Forest Regressor\n",
        "rfr = RandomForestRegressor()\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=rfr, param_grid=param_grid, scoring='r2', cv=5)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and best estimator\n",
        "best_params = grid_search.best_params_\n",
        "best_estimator = grid_search.best_estimator_\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(best_params)\n",
        "\n",
        "# Re-fit the model with the best estimator\n",
        "best_estimator.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the model with the best estimator\n",
        "y_pred_best = best_estimator.predict(x_test)\n",
        "\n",
        "# Evaluate the performance of the tuned model\n",
        "get_metrics(y_test, y_pred_best, \"RandomForest (Tuned)\")\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV is a widely used technique for hyperparameter tuning. It exhaustively searches through a specified parameter grid and evaluates the model's performance for each combination of hyperparameters using cross-validation. It helps to find the best set of hyperparameters that yield the highest performance metric (in this case, the R-squared score) on the validation data."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the provided evaluation metric scores, the performance of the RandomForestRegressor model improved slightly after hyperparameter tuning:\n",
        "\n",
        "Original RandomForestRegressor:\n",
        "- MSE: 39426.483\n",
        "- RMSE: 198.561\n",
        "- MAE: 120.252\n",
        "- R2 Score: 0.903\n",
        "\n",
        "Tuned RandomForestRegressor:\n",
        "- MSE: 38411.585\n",
        "- RMSE: 195.989\n",
        "- MAE: 118.588\n",
        "- R2 Score: 0.905\n",
        "\n",
        "Although the metrics show a slight increase in MSE, RMSE, and MAE, the R2 score remains almost the same. This indicates that the tuned RandomForestRegressor model's performance is comparable to the original model, with a marginal change in some metrics."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4 XGBoost"
      ],
      "metadata": {
        "id": "EkhXis39jr4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "from xgboost import XGBRegressor\n",
        "# Fit the Algorithm\n",
        "xgbr = XGBRegressor().fit(x_train,y_train)\n",
        "y_pred_xgbr = xgbr.predict(x_test)\n",
        "# Predict on the model\n",
        "get_metrics(y_test,y_pred_xgbr,\"XGB\")\n"
      ],
      "metadata": {
        "id": "BaBs9-2aj01j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "WCWXIGhSkarp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.scatter(y_test,y_pred_xgbr)\n",
        "plt.xlabel('Ground Truth')\n",
        "plt.ylabel('Prediction')\n",
        "plt.title('XGboost')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nz_GMQR7nioF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "AyV34IJHk4ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Fit the Algorithm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],  # Number of trees\n",
        "    'learning_rate': [0.1, 0.01, 0.001],  # Learning rate\n",
        "    'max_depth': [3, 4, 5],  # Maximum depth of each tree\n",
        "    'min_child_weight': [1, 2, 3]  # Minimum sum of instance weight (hessian) needed in a child\n",
        "}\n",
        "\n",
        "# Create an instance of XGBRegressor\n",
        "xgbr = XGBRegressor()\n",
        "\n",
        "# Create an instance of GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgbr, param_grid=param_grid, scoring='r2', cv=5)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and best estimator\n",
        "best_params = grid_search.best_params_\n",
        "best_estimator = grid_search.best_estimator_\n",
        "\n",
        "# Predict on the test set using the best estimator\n",
        "y_pred_best = best_estimator.predict(x_test)\n",
        "\n",
        "# Evaluate the performance of the tuned model\n",
        "get_metrics(y_test, y_pred_best,\"XGB(Tuned)\")\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "kit1L47IlDeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "params={'max_length':[3,5,6,10,15,20],\n",
        "        'learning_rate':[0.01,0.1,0.2,0.3],\n",
        "        'subsample':np.arange(0.4,1.0,0.1),\n",
        "        'colsample_bytree':np.arange(0.4,1.0,0.1),\n",
        "        'colsample_bylevel':np.arange(0.4,1.0,0.1),\n",
        "        'n_estimators':[100,500,1000]\n",
        "        }\n",
        "\n",
        "xgbr=XGBRegressor(seed=20)\n",
        "rscv=RandomizedSearchCV(estimator=xgbr,\n",
        "                        param_distributions=params,\n",
        "                        scoring='neg_mean_squared_error',\n",
        "                        n_iter=25,\n",
        "                        cv=5,\n",
        "                        verbose=1\n",
        "                        )\n",
        "\n",
        "rscv.fit(x_train,y_train)\n",
        "\n",
        "y_pred_xgb_random=rscv.predict(x_test)\n",
        "get_metrics(y_test,y_pred_xgb_random,'XGB with RandomizedSearchCV')\n",
        "\n",
        "# Best parameters\n",
        "print(rscv.best_params_)"
      ],
      "metadata": {
        "id": "S8TGQpZ6fRgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PYWD5ggplOId"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameter optimization technique used in this code snippet is RandomizedSearchCV. RandomizedSearchCV is used when we have a large hyperparameter space and want to randomly sample a subset of combinations for evaluation. It is more efficient than GridSearchCV because it does not exhaustively search all possible combinations."
      ],
      "metadata": {
        "id": "bhsQJ4BYmdtI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "99NjV6qjmmUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, hyperparameter tuning using RandomizedSearchCV has led to improvements in the model's performance. Here is the updated evaluation metric score chart:\n",
        "\n",
        "xgboost (Before Hyperparameter Tuning):\n",
        "\n",
        "MSE: 32995.73\n",
        "\n",
        "RMSE: 181.647\n",
        "\n",
        "MAE: 111.896\n",
        "\n",
        "R2: 0.919\n",
        "\n",
        "xgboost (After Hyperparameter Tuning):\n",
        "\n",
        "MSE: 26546.79\n",
        "\n",
        "RMSE: 162.932\n",
        "\n",
        "MAE: 100.318\n",
        "\n",
        "R2: 0.935\n",
        "\n",
        "After hyperparameter tuning, the xgboost model shows improvements in all evaluation metrics. The MSE, RMSE, and MAE have decreased, indicating better prediction accuracy, while the R2 score has increased, indicating a better fit to the data."
      ],
      "metadata": {
        "id": "SOzHOg2KnN-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When considering the evaluation metrics for a positive business impact, the following metrics are typically important:\n",
        "\n",
        "Mean Squared Error (MSE): MSE measures the average squared difference between the predicted and actual values. A lower MSE indicates better accuracy in predicting the target variable. By minimizing MSE, the model aims to reduce the overall prediction errors, which is beneficial for businesses that rely on precise predictions.\n",
        "\n",
        "Root Mean Squared Error (RMSE): RMSE is the square root of MSE and provides a measure of the average magnitude of the prediction errors. Similar to MSE, a lower RMSE indicates better accuracy. RMSE is commonly used in cases where the prediction errors are expected to be on a similar scale as the target variable.\n",
        "\n",
        "Mean Absolute Error (MAE): MAE calculates the average absolute difference between the predicted and actual values. It provides a measure of the average magnitude of the prediction errors, regardless of their direction. Like MSE and RMSE, a lower MAE indicates better accuracy. MAE is useful when the business wants to understand the average deviation of the predictions from the actual values.\n",
        "\n",
        "R-squared (R2) Score: R2 score represents the proportion of the variance in the target variable that is predictable from the independent variables. It indicates the goodness of fit of the model and ranges between 0 and 1. A higher R2 score signifies that a larger proportion of the target variable's variance is explained by the model. A high R2 score is desirable as it suggests that the model is capturing the underlying patterns in the data."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The XGBoost (XGB) model seems to have the best performance among the four models considered. It has the lowest values of mean squared error (MSE), root mean squared error (RMSE), and mean absolute error (MAE), indicating better accuracy and precision in its predictions. Additionally, it has the highest value of the coefficient of determination (R-squared), indicating a better fit to the data compared to the other models.\n",
        "\n",
        "Therefore, the XGBoost model is a suitable choice as the final prediction model. XGBoost is a powerful and popular machine learning algorithm that combines the advantages of gradient boosting and decision trees. It is known for its ability to handle complex relationships and capture non-linear patterns in the data. With its strong performance and robustness, the XGBoost model is likely to provide more accurate predictions and better generalization on unseen data."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this scenario, the XGBoost (XGB) model was chosen as the final prediction model. XGBoost is a popular machine learning algorithm that utilizes gradient boosting and decision trees to achieve high predictive accuracy.\n",
        "\n",
        "XGBoost works by iteratively adding decision trees to the ensemble, where each subsequent tree is built to correct the mistakes of the previous trees. The algorithm optimizes a loss function by minimizing it through gradient descent. It also incorporates regularization techniques to control overfitting and enhance generalization.\n",
        "\n",
        "To explain the feature importance in the XGBoost model, we can utilize the built-in feature importance attribute provided by XGBoost. This attribute calculates the relative importance of each feature in the model based on how much each feature contributes to reducing the loss function.\n",
        "\n",
        "Additionally, there are several model explainability tools that can be used to gain further insights into the XGBoost model. Some popular tools include SHAP (SHapley Additive exPlanations), ELI5 (Explain Like I'm 5), and LIME (Local Interpretable Model-Agnostic Explanations). These tools provide various techniques for interpreting the model's predictions, understanding feature contributions, and identifying the key factors influencing the output.\n",
        "\n",
        "By utilizing these model explainability tools, we can gain a deeper understanding of how the XGBoost model makes predictions, which features are most influential in the decision-making process, and potentially uncover any non-linear relationships or interactions between the features.\n",
        "\n",
        "It's important to note that the specific tool and its usage may depend on the available resources, requirements, and the complexity of the problem at hand. Each tool has its own set of functionalities and techniques for model explainability."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "dir = \"/content\"  # Directory where the file will be saved\n",
        "model_file_name = \"xgboost_regressor_r2_0_935_v1.pkl\"  # Name of the pickle file\n",
        "model_file_path = os.path.join(dir, model_file_name)\n",
        "\n",
        "# Assuming 'xgbr' is the best performing XGBoostRegressor model\n",
        "pickle.dump(xgbr, open(model_file_path,'wb'))"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "# Assuming 'model_file_path' contains the path to the saved pickle file\n",
        "loaded_model = pickle.load(open(model_file_path, 'rb'))\n",
        "\n",
        "# Assuming 'x_test' contains the unseen data for prediction\n",
        "predictions = loaded_model.fit(x_train, y_train).predict(x_test)\n",
        "\n",
        "# Print the predictions\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#unseen data\n",
        "date=\"01/07/2023\"\n",
        "hour=21\n",
        "temperature=25\n",
        "humidity=55\n",
        "wind_speed=67\n",
        "visibility=25\n",
        "solar_radiation=0.0\n",
        "rainfall=0.7\n",
        "snowfall=0.2\n",
        "seasons='Winter'\n",
        "holiday='No Holiday'\n",
        "functioning_day='Yes'"
      ],
      "metadata": {
        "id": "Kmt6k--RVWvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "#converting data into model consumable format\n",
        "holiday_dict={'No Holiday':0,'Holiday':1}\n",
        "functioning_dict={'No':0,'Yes':1}\n",
        "\n",
        "def get_str_to_datetime(date):\n",
        "  dt=datetime.strptime(date ,\"%d/%m/%Y\")\n",
        "  return {\"day\":dt.day,\"month\":dt.month,\"year\":dt.year,\"week_day\":dt.strftime(\"%A\")}\n",
        "\n",
        "str_to_date=get_str_to_datetime(date)\n",
        "str_to_date\n"
      ],
      "metadata": {
        "id": "wKBKraLVC-6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_input_list=[hour,temperature,humidity,wind_speed,visibility,solar_radiation,rainfall,snowfall,\n",
        "                 holiday_dict[holiday],functioning_dict[functioning_day],\n",
        "                 str_to_date[\"day\"],str_to_date[\"month\"],str_to_date[\"year\"]]"
      ],
      "metadata": {
        "id": "ONN2cok8bKJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_name=['Hour', 'Temperature(°C)', 'Humidity(%)',\n",
        "       'Wind speed (m/s)', 'Visibility (10m)', 'Solar Radiation (MJ/m2)',\n",
        "       'Rainfall(mm)', 'Snowfall (cm)', 'Holiday', 'Functioning Day', 'Day',\n",
        "       'Month', 'Year']"
      ],
      "metadata": {
        "id": "9o_whBmVfZRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seasons_to_df(seasons):\n",
        "  seasons_col=['Spring','Summer','Winter']\n",
        "  seasons_data=np.zeros((1,len(seasons_col)),dtype='int')\n",
        "\n",
        "  df_seasons=pd.DataFrame(seasons_data,columns=seasons_col)\n",
        "  if seasons in seasons_col:\n",
        "    df_seasons[seasons]=1\n",
        "    return df_seasons\n",
        "\n",
        "df_seasons=seasons_to_df(seasons)\n",
        "df_seasons"
      ],
      "metadata": {
        "id": "twwmwSYtfW94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def days_df(week_day):\n",
        "  days_names=['Monday','Saturday','Sunday','Thursday','Tuesday','Wednesday']\n",
        "  days_name_data=np.zeros((1,len(days_names)),dtype='int')\n",
        "\n",
        "  df_days=pd.DataFrame(days_name_data,columns=days_names)\n",
        "\n",
        "  if week_day in days_names :\n",
        "    df_days[week_day]=1\n",
        "    return df_days\n",
        "\n",
        "df_days=days_df(str_to_date['week_day'])\n",
        "df_days"
      ],
      "metadata": {
        "id": "S5bR_sj0iHlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_user_input=pd.DataFrame([user_input_list],columns=feature_name)"
      ],
      "metadata": {
        "id": "Mbn74HdKjwcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unseen_data=pd.concat([df_user_input,df_seasons,df_days],axis=1)"
      ],
      "metadata": {
        "id": "nBmT6oXlmx7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unseen_data"
      ],
      "metadata": {
        "id": "K3gg7_1voiO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc_unseen_data=sc.transform(unseen_data)\n",
        "sc_unseen_data"
      ],
      "metadata": {
        "id": "VWtWqJRtoXNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = loaded_model.predict(sc_unseen_data)\n",
        "print(f\"Rented bike demand on date {date}, and hour {hour} is: {round(x.tolist()[0])}\")"
      ],
      "metadata": {
        "id": "3W1QVdBzp1CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In conclusion, the machine learning project for the Seoul bike sharing dataset successfully developed and evaluated several models to predict the demand for bike rentals. The XGBoost Regression model emerged as the best performing model, achieving an impressive R2 score of 0.935. This indicates that the model can explain 93.5% of the variance in the bike rental demand.\n",
        "\n",
        "The project highlighted the importance of feature engineering and selection, as well as hyperparameter tuning, in improving the models' performance. The tuned models, particularly the tuned Decision Tree Regression and Random Forest Regression models, showcased improvements in their R2 scores.\n",
        "\n",
        "The evaluation metrics, including MSE, RMSE, MAE, and R2 Score, provided valuable insights into the accuracy and precision of the models. These metrics can guide bike-sharing companies in assessing the performance of their systems and making data-driven decisions to optimize their operations.\n",
        "\n",
        "The developed machine learning models have significant potential for the bike-sharing industry in Seoul. By accurately predicting bike rental demand, companies can allocate resources efficiently, manage inventory, and improve customer satisfaction. These models enable data-driven decision-making, which can lead to cost savings and an enhanced user experience.\n",
        "\n",
        "However, it is important to note that the success of the models relies on the availability of high-quality data and the periodic retraining and updating of the models as new data becomes available. Regular model evaluation and monitoring are essential to ensure their continued effectiveness.\n",
        "\n",
        "Overall, the machine learning project demonstrates the value of predictive modeling in the bike-sharing industry. The models provide actionable insights that can drive business decisions and optimize operations, ultimately contributing to the success of bike-sharing companies and improving the overall biking experience for users in Seoul."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}